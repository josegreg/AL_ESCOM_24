---
title: "Sistemas de ecuaciones, eliminación gaussiana"
author: "Jose Rodriguez Villarreal"
header-includes:
  -\usepackage[latin1]{inputenc}
  -\usepackage{mathtools}
output:
  slidy_presentation:
    css: 'styles.css'
    footer: "Álgebra lineal, ESCOM 2BV1, José Rodríguez Villarreal"
---
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

# Sistemas de ecuaciones


## Introducción

En $\mathbb{R}^2$, un sistema de ecuaciones lineales consiste en encontrar un *vector* $\vec{x}=(x_1,x_2)$, tal que

\begin{equation}
\begin{array}{rl}
 a_{11}x + a_{12}y =& b_1 \\
 a_{21}x + a_{22}y =& b_2 
\end{array} 
\end{equation}

Ver <a href="https://www.geogebra.org/m/e7hzJEGf">Ejemplo</a>

Como se puede ver, la existencia de una solución corresponde a la existencia de un cruce entre las rectas que representan cada ecuación. 
En otros casos no existe una solución, es decir, no existe un cruce.

**Observación** Esto depende de los coeficientes de las rectas, así como del  vector independiente.




## Sistemas de ecuaciones 2x2

En $\mathbb{R}^2$, es decir, en el plano cartesiano un sistema de ecuaciones consiste en 


  I.   Un conjunto de números, llamados _coeficientes_ $a_{11},a_{12},a_{21},a_{22}$.
  II.  Un vector $\vec{\mathbf{b}}=(b_1,b_2)$ llamado _término_ _independiente_.
  III. Un vector $\vec{S}=(x_1,x_2)$ llamado el _vector_ _solución_ ó _solución_

### Preguntas

¿Bajo que condiciones se tiene que el sistema tiene solución única?
¿Bajo que condiciones se tiene que el sistema **no** tiene solución única?

<div class="def">
  * Si las ecuaciones representan líneas paralelas entonces el sistema no tiene solución
  * Si las ecuaciones representa a la misma línea entonces las soluciones son todos los puntos de la recta.
  * Si las ecuaciones representan líneas no paralelas entonces el sistema tiene una única solución
</div>

Suponga que se tiene un sistema de ecuaciones, ningua de las rectas en el sistema es vertical,

$$
\begin{array}{cc}
ax + by = & e \\
cx + dy = & f
\end{array}
$$

Si $l_1$ es la recta con ecuación $ax+by=e$ y $l_2$ es la recta con ecuación $cx+dy=f$ entonces las rectas  serán paralelas si sus pendientes son iguales, es decir $m_1 = m_2$, o bien
\[
\frac{a}{b}  =  \frac{c}{d}
\]

Dicho de otra forma

<div class="def">
Considere un sistema de ecuaciones en $\mathbb{R}^2$, 

\[
\begin{array}{cc}
  ax + by = & e \\
  cx + dy = & f
\end{array}
\]

de tal forma que $b\ne 0$ o $d\ne 0$ entonces, las rectas son paralelas si
\[
\Delta=ad-bc = 0
\]
</div>
 



## Sistemas de ecuaciones $3\times 3$

Un sistema de ecuaciones lineales $3\times 3$ es de la siguiente forma:
$$
\begin{matrix}
a_{11}x_1+a_{12}x_2 + a_{13}x_3 = & b_1\\
a_{21}x_1+a_{22}x_2+ a_{23}x_3  = & b_2\\
a_{31}x_1+a_{32}x_2+ a_{33}x_3  = & b_3
\end{matrix}
$$
 
donde $a_{ij},b_i\in\mathbb{R},\ i=1,2$ y $j=1,2$ suelen ser constantes conocidas.

* A los números $a_{11},\,a_{12},\,a_{13}, a_{21},\, a_{22},\,a_{23},a_{31},\,a_{32},\, a_{33}$ se les llama _coeficientes_ del sistema

* A los escalares $b_1,\,b_2,\,\ldots,b_n$ es el término independiente.

* Al vector $\vec{\mathbf{x}}=(x_1,x_2,\ldots,x_n)$ es el vector incógnita.

Al sistema de ecuaciones lineales de la forma 

\begin{equation}
\begin{matrix}
 a_{11}x_1+a_{12}x_2 + a_{13}x_3 = & 0\\
 a_{21}x_1+a_{22}x_2+ a_{23}x_3  = & 0\\
 a_{31}x_1+a_{32}x_2+ a_{33}x_3  = & 0
\end{matrix}
\end{equation}

Se dice ser un sistema _homogéneo_. En caso contrario, es _no_ _homogéneo_.

## Operaciones elementales

1) Intercambiar una ecuación con otra ecuación

2) Multiplicar una ecuación por un escalar

3) Sumar a una ecuación un múltiplo de otra ecuación

Una ecuación del tipo $a_{11} x_1 + a_{12} x_2 + a_{13} x_3 =  b_1$ se dice que es una ecuación lineal en las incógnitas $x_1,\,x_2\,x_3$.

Una solución de sistema de ecuaciones lineales es un vector $\vec{x}=(s_1,s_2,s_3)$ tal que la igualdad se cumple _de_ _manera_ _simultánea_ cuando se hace la sustitución $x_1=s_1,\,x_2=s_2,\,x_3=s_3$. El conjunto de todas las soluciones de la ecuación es su **conjunto** **solución** $\mathcal{X}$.



# Sistemas de ecuaciones II

Cada ecuación en el sistema de ecuaciones $3\times 3$ representa a un plano en $\mathbb{R}^{3}$

$$
\begin{matrix}
a_{11}x_1+a_{12}x_2 + a_{13}x_3 = & b_1\\
a_{21}x_1+a_{22}x_2+ a_{23}x_3  = & b_2\\
a_{31}x_1+a_{32}x_2+ a_{33}x_3  = & b_3
\end{matrix}
$$

El conjunto solución $\mathcal{X}$ consiste en todos los puntos que tienen en común los 3 planos _simultaneamente_.


## Ejemplo 1

Resolver el sistema
$$
\begin{matrix}
x_1 & - 2 x_2 & + 3 x_3     & = 9\\
-x_1& + 3x_2  &             & = -4\\
2x_1&-5x_2    & + 5x_3      & = 17
\end{matrix}
$$



# Eliminación gaussiana 
\small
Analizamos a fondo el método de eliminación gaussiana.
Considere el sistema de ecuaciones

\begin{equation}
  \begin{matrix}
     a_{11} x_1 & +a_{12} x_2 + & \cdots   & +a_{1n} x_n  &= b_1  \\
     a_{21} x_1 & +a_{22} x_2 + & \cdots   & +a_{2n} x_n  &= b_2\\
    \vdots    & \vdots  & \vdots   & \vdots \\
     a_{n1} x_1 & +a_{n2} x_2 + & \cdots   & +a_{nn} x_n  & =b_n
  \end{matrix}
\end{equation}

Suponga que $a_{11}\ne 0$ entonces podemos eliminar $x_1$ de las ecuaciones $i=2,\ldots,n$, para la $i-$ésima ecuación, multiplicando la primer ecuación por

\begin{equation*}
m_{i1}=\frac{a_{i1}}{a_{11}}, \mbox{para }i=2,\ldots,n
\end{equation*}
y sumarla a la ecuación $i$.

Obtenemos el siguiente sistema

\begin{equation}
  \begin{matrix}
     a_{11} x_1 & +a_{12} x_2      + & \cdots   & +a_{1n} x_n  &= b_1  \\
                & a^{(2)}_{22} x_2 + & \cdots   & +a^{(2)}_{2n} x_n  &= b^{(2)}_2\\
               & \vdots  & \vdots   & \vdots \\
               & +a^{(2)}_{n2} x_2 + & \cdots   & +a^{(2)}_{nn} x_n  & =b^{(2)}_n
  \end{matrix}
\end{equation}
 
Con $a^{(2)}_{ij} = a_{ij}- m_{i1}a_{1j}$ y $b^{(2)}_i=b_i-m_{i1}b_1$ los nuevos coeficientes del bloque de ecuaciones inferior de tamaño $(n-1)\times(n-1)$.
En este paso hemos reducido la primer columna del sistema (salvo el pivote) a $0$.

Ahora aplicamos el mismo proceso al sistema de ecuaciones  $(n-1)x(n-1)$, 

Si $a^{(2)}_{22}\ne 0$ entonces podemos eliminar $x_2$ de las ecuaciones $i=3,\ldots,n$ multiplicando la segunda ecuación por

\begin{equation*}
m_{i2}=\frac{a^{(2)}_{i2}}{a^{(2)}_{22}}, \mbox{para }i=3,\ldots,n
\end{equation*}

y restarla a la ecuación $i$.


Obtenemos el siguiente sistema

\begin{equation}
  \begin{matrix}
     a_{11} x_1 & +a_{12} x_2      + & a_{13}x_3 +      & \cdots   & +a_{1n} x_n  &= b_1  \\
                & a^{(2)}_{22} x_2 + & a^{(2)}_{23}x_3 +& \cdots   & +a^{(2)}_{2n} x_n  &= b^{(2)}_2\\
                &   0               +& a^{(3)}_{33}x_3 +& \cdots   & +a^{(3)}_{2n} x_n  &= b^{(3)}_3\\
                & \vdots             & \vdots&          & \vdots   &    \vdots          & \vdots\\
                &    0              +& a^{(3)}_{n3}x_3 +& \cdots   & +a^{(3)}_{nn} x_n  & =b^{(3)}_n
  \end{matrix}
\end{equation}


Procedemos, si es posible, de la misma forma.

Después de $k-1$ pasos  si $a^{(k-1)}_{kk}\ne 0$ entonces podemos eliminar $x_k$ de las ecuaciones $i=k+1,\ldots,n$ multiplicando la segunda ecuación por

\begin{equation*}
m_{ik}=\frac{a^{(k-1)}_{ik}}{a^{(k-1)}_{kk}}, \mbox{para }i=k+1,\ldots,n
\end{equation*}
y sumarla a la ecuación $i$.



Procedemos así hasta obtener el sistema triangular

\begin{equation}
  \begin{matrix}
     a_{11} x_1 & +a_{12} x_2      + & a_{13}x_3 +      & \cdots   & +a_{1n} x_n        &= b_1  \\
                & a^{(2)}_{22} x_2 + & a^{(2)}_{23}x_3 +& \cdots   & +a^{(2)}_{2n} x_n  &= b^{(2)}_2\\
                &   0               +& a^{(3)}_{33}x_3 +& \cdots   & +a^{(3)}_{2n} x_n  &= b^{(3)}_3\\
                & \vdots             & \vdots           & \vdots   &    \vdots          & \vdots\\
                &                    &  0                & \cdots   & +a^{(n-1)}_{n-1n} x_n  & =b^{(n-1)}_{n-1}\\
                &                    & 0                 & \cdots   & +a^{(n-1)}_{nn} x_n  & =b^{(n-1)}_n
  \end{matrix}
\end{equation}

## Sustitución hacia atrás

Para $k =n$
$$
x_n = \frac{b^{(n-1)}_n}{a^{(n-1)}_{nn}}
$$

Para $k=n-1$

$$
x_{n-1} = \frac{b^{(n-1)}_{n-1} - a^{(n-1)}_{n-1,n}x_n}{a^{(n-1)}_{n-1,n-1}}
$$

Y
$$
x_{n-2} = \frac{b^{(n-2)}_{n-2}- a^{(n-2)}_{n-2,n-1}x_{n-1} - a^{(n-2)}_{n-2,n}x_n}{a^{(n-2)}_{n-2,n-2}}
$$

Y en general

$$x_{k}=\frac{b^{(k)}_k-\sum_{i=k+1}^{n}a^{(k)}_{kj}x_j}{a^{(k)}_{kk}}$$


## Algoritmo de eliminación gaussiana

El algoritmo en pseudocódigo:

```{verbatim}
for k= 1, ..., n-1:{

  If A(k,k)=0:  
    proc: "intercambiar renglones"
    # A(k,k,) es el pivote
    for i=k+1,...,n:{
      m(i,k) = A(i,k)/A(k,k)            #Calculamos el multiplicador del rengĺón pivotal
      for j=k+1,...,n:{                 #Modifcamos a las columnas del renglón i
        A(i,j)  = A(i,j)-m(i,k)*A(k,j)
        b(i)    = b(i)-m(i,k)b(k)
      }
    }
}
```

Si es posible obtener un sistema triangular entonces, realizamos la sustitución hacia atrás

### Algoritmo de sustitución hacia atrás

```{verbatim}
If a(n,n)=0:
    no existe solución única
Else:
  for k=n-1,n-1,...,1
    suma = b(k)
    for j=k+1,...,n
      suma = suma - a(k,j)*x(j)
    x(k) = suma/a(k,k)
```

## Ejemplo

Aplicar el algoritmo al sistema


\begin{equation*}
\begin{array}{ccc}
  \begin{matrix}
           4x  & -9 y  & +2z & = 2  \\
           2x & -4 y  & +4z  & = 3  \\
           -x & +2 y  & +2z  & = 1´
  \end{matrix}
&  
\xrightarrow{E_1\leftrightarrow E_2}
&
\begin{matrix}
           -x & +2 y  & +2z  & = 1  \\
           2x & -4 y  & +4z  & = 3  \\
           4x  & -9 y  & +2z & = 2  
  \end{matrix}
\end{array}
\end{equation*}

\begin{equation*}
\begin{array}{ccc}
  \begin{matrix}
           -x & +2 y  & +2z  & = 1  \\
              &       & +8z  & = 5  \\
           4x & -9 y  & +2z  & = 2  
  \end{matrix}
&  
\xrightarrow{4E_1 + E_3\rightarrow E_3}
&
\begin{matrix}
           -x & +2 y  & +2z  & = 1  \\
              &       & +8z  & = 5  \\
              & -y    & +10z & =6  
  \end{matrix}
\end{array}
\end{equation*}


\begin{equation*}
\begin{array}{ccc}

\xrightarrow{E_2 \leftrightarrow E_3}
&  
\begin{matrix}
           -x & +2 y  & +2z  & = 1  \\
              & -y    & +10z & = 6   \\
              &       & +8z  & = 5  
\end{matrix}
&
\begin{matrix}
            x &   &      & = 3/4        \\
              & y &      & = 10(5/8) -6=1/4   \\
              &   &   z  & = 5/8  
\end{matrix}
\end{array}
\end{equation*}

La solución es única y es igual a $x=\frac{3}{4}$, $y=\frac{1}{4}$, $z=\frac{5}{8}$, o bien $\mathcal{X}=\begin{Bmatrix}\,(\frac{3}{4},\frac{1}{4},\frac{5}{8})\,\,\end{Bmatrix}$.


# Sistemas consistentes e inconsistentes

## Definición 

Un sistema de ecuaciones es **consistente** si tiene al menos una solución. Si no es consistente entonces se dice ser **inconsistente**.

Dos sistemas son **equivalentes** si tienen exactamente al mismo conjunto solución. 
Un sistema de ecuaciones puede ser consistente sin tener una única solución.


# Uso de matrices


\small
Como el procedimiento de eliminación gaussiana sólo involucra a los coeficientes, una forma de simplificar los cálculos es por medio de un arreglo que llamaremos **Matriz**.
La matriz asociada al sistema de ecuaciones:



### Ejemplo 2


Resolver el siguiente sistema de ecuaciones, aplicando eliminación Gaussiana.


\begin{equation*}
  \begin{matrix}
            &   v  & - w   & = 3  \\
     -2 u   & + 4v & - w   & = 1   \\
     -2 u   & + 5v &  -4w  & =-2
  \end{matrix}
\end{equation*}


\begin{equation*}
  \begin{bmatrix}
              0   &   1 & - 1  &\biggm| & 3  \\
             -2   &   4 & - 1  &\biggm| & 1   \\
             -2   &   5 &  -4  &\biggm| &-2
  \end{bmatrix}
  \rightarrow
    \begin{bmatrix}
              -2  &   5 & - 4  &\biggm| &-2   \\
             -2   &   4 & - 1  &\biggm| & 1   \\
              0   &   1 & -1   &\biggm| & 3
  \end{bmatrix}
\end{equation*} 

\begin{equation*}
  \rightarrow
    \begin{bmatrix}
              -2  &   5 & - 4  &\biggm| &-2   \\
              0   & -1  &   3  &\biggm| & 3   \\
              0   &   1 & -1   &\biggm| & 3
  \end{bmatrix} \rightarrow
    \begin{bmatrix}
              -2  &   5 & - 4  &\biggm| &-2   \\
              0   & -1  &   3  &\biggm| & 3   \\
              0   &  0  &   2   &\biggm| & 6
  \end{bmatrix} 
\end{equation*}

Regresamos al sistema asociado a esta matriz extendida

\begin{equation*}
\begin{matrix}
      -2u   &  +5v  & -4 w   & = -2  \\
            &  -v   &  +3w   & = 3   \\
            &       &  2w    & =6
  \end{matrix}
\end{equation*}

y se obtiene la solucion $\vec{x}=(10,6,3)$

### Ejemplo 3


\begin{equation*}
  \begin{matrix}
        w   & - x  & - y  & +2z & = 1  \\
     2 w    & - 2x & - y  & +3z & = 3   \\
      -w    & + x  &  -y  &     &=-3
  \end{matrix}
\end{equation*}

Es igual a

\begin{equation*}
  \begin{bmatrix}
        1   & - 1 & - 1  & 2   \\
        2   & - 2 & - 1  & 3    \\
      - 1   &   1 &  -1  & 0    
  \end{bmatrix}
\end{equation*}

Existe otra matriz asociada, la **matriz** **extendida**


\begin{equation*}
 \begin{array}{cc}
  [\mathbf{A}| \mathbf{b}] & = \begin{bmatrix}
                                  1   & - 1 & - 1  & 2 &\biggm| & 1  \\
                                  2   & - 2 & - 1  & 3 &\biggm| & 3   \\
                                 -1   &   1 &  -1  & 0 &\biggm| &-3
                                \end{bmatrix}
 \end{array}
\end{equation*} 


### Ejemplo 

Encontrar el valor de $m$ tal que el siguiente sistema tenga una solución única

\begin{equation*}
  \begin{matrix}
        m x  & -5 y  & +2z & = 0  \\
           x & - y  & +3z & = 0   \\
      3 x    &  -7y & -5z &=0 
  \end{matrix}
\end{equation*}

El método de eliminación gaussiana, también nos permite determinar si un sistema es **inconsistente**.

### Ejemplo 

Encontrar el conjunto solución

\begin{equation*}
  \begin{matrix}
         &  y  & -4z &  = 8  \\
       2x & - 3y   & +2z & = 1   \\
       4x &  -8y   & +12z & =1 
  \end{matrix}
\end{equation*}




## Matrices

A los arreglos rectangulares con las listas de coeficientes $a_{ij}$ de cada ecuación, se les llama **matrices**. Formalmente, una matriz es una asignación de coeficientes a cada "variable-ecuación" del sistema de ecuaciones.

\begin{equation*}
\mathbf{A} =  \begin{pmatrix}
                a_{11}  & a_{12}  & \cdots   & a_{1n} \\
                a_{21}  & a_{22}  & \cdots   & a_{2n} \\
                \vdots   & \vdots  & \vdots   & \vdots \\
                a_{m1}  & a_{m2}  & \cdots   & a_{mn} 
              \end{pmatrix}
\end{equation*}

De manera alternativa una matriz se puede simbolizar de la forma $\mathbf{A} = (a_{ij})_{i\leq m,j\leq n}$. Para referirnos al número en la ecuación $i$, variable $j$ es $a_{ij}$. En el contexto de matrices, nos referimos a los coeficientes de la ecuación $i$ como _fila_ $i$ y los coeficientes de la variable $x_j$ como columna $j$.


## Forma escalonada.

### Definicion

Una matriz se dice que tiene la forma escalonada si las siguientes condiciones se cumplen

  a) Todas las filas con ceros estan abajo de las filas no nulas.

  b) En cada renglón no nulo, la primer entrada no nula está a la izquierda de la primer entrada no nula del siguiente renglón.

## Ejemplo

\begin{equation*}
\begin{array}{ccc}
a) \begin{bmatrix}
      1   &  2  &   3     \\
      0   &   1 &   0  
\end{bmatrix}
&
b) \begin{bmatrix}
      1   &  2  &   3     \\
      0   &   0 &   4     \\
      0   &   1 &   -2     
\end{bmatrix}
&
c) \begin{bmatrix}
      2   &   0 &  -1  &  5  \\
      0   &   0 &   3  &  2  \\
      0   &   0 &   0  &  0  
\end{bmatrix}
\end{array}
\end{equation*} 

Las matrices $a)$ y $c)$ tienen la forma escalonada, la matriz $b)$ no están en su forma escalonada. 

## Ejemplo 

Considere el  sistema de ecuaciones

\begin{equation*}
  \begin{matrix}
           x  & +2 y  & +3z & = 1  \\
           3x & +5 y  & -2z & = 2   \\
           4x & +7 y  & +z  & = 3
  \end{matrix}
\end{equation*}
 Aplicar eliminación gaussiana para obtener un sistema triangular o escalonado

$$
\begin{array}{ccc}
  E_2 \rightarrow -3E_1 + E_2 & & \\
  x + 2y +3z  & = &  1 \\
  0x  -y -11z & = & -1 \\
  4x +7y +z  & = & 3
\end{array}
$$

$$
\begin{array}{ccc}
  E_3 \rightarrow -4E_1 + E_3 & & \\
  x + 2y +3z  & = &  1 \\
  0x  -y -11z & = & -1 \\
  0x -7y -11z & = & -1
\end{array}
$$

Ahora sumamos $-2E_1$ a la segunda ecuación 

$$
\begin{array}{ccc}
  E_1 \rightarrow -2E_2 + E_1 & & \\
  x + 0y -19z  & = &  -1 \\
  0x + y +11z  & = &  1 \\
  0x -7y +11z & = & -1
\end{array}
$$


Ahora sumamos $7E_2+E_3 \rightarrow E_3$ a la segunda ecuación 

\begin{equation}
\begin{array}{ccc}
  E_1 \rightarrow -2E_2 + E_1 & & \\
  x + 0y  -19z  & = &  -1 \\
  0x + y  +11z  & = &  1 \\
  0x + 0y +88z  & = & -6
\end{array}
\end{equation}


Entonces una forma escalonada _equivalente_ al sistema inicial es 

\begin{equation*}
  \begin{bmatrix}
        1   &   0 & - 19  &| & -1   \\
        0   &   1 &  11   &| &1    \\
        0   &   0 &  88   &| &-6    
  \end{bmatrix}
\end{equation*}

## Ejemplo

Resolver el sistema _homogéneo_ usando la matriz aumentada del sistema

\begin{equation*}
\begin{matrix}
-x_1 & + 3 x_2  & + 3 x_3 &  + 2 x_4   & = 0\\
2x_1 &          & + 6 x_3 &  + x_4     & = 0\\
-2x_1& +4x_2    & + 2 x_3 &  + 4 x_4   & = 0
\end{matrix}
\end{equation*}

La matriz aumentada del sistema es igual a

\begin{equation*}
  \begin{bmatrix}
      - 1   & 3   &   3  & 2  & \bigm| & 0   \\
        2   & 0   &   6  & 1  & \bigm| & 0    \\
      - 2   & 4   &   2  & 4  & \bigm| & 0  
  \end{bmatrix}
\end{equation*}

Aplicando la operacion elemental  $2E_1+E_2\rightarrow E_2$ y $-2E_1+E_3 \rightarrow E_3$

\begin{equation*}
\begin{array}{ccc}
  \begin{bmatrix}
           -1  &  3  & 3  & 2  & \bigm| & 0  \\
            0  &  6  & 12 & 5  & \bigm| & 0  \\
            0  &  -2 & -4 & 0  & \bigm| & 0
  \end{bmatrix}
&  
\xrightarrow{2/6E_2 + E_3\rightarrow E_3}
&
\begin{bmatrix}
           -1  &  3  & 3  & 2           & \bigm| & 0  \\
            0  &  6  & 12 & 5           & \bigm| & 0  \\
            0  &  0  & 0  & \frac{5}{3} & \bigm| & 0
\end{bmatrix}
\end{array}
\end{equation*}


\begin{equation*}
\begin{array}{ccc}
\xrightarrow[-E_1 \rightarrow E_1]{1/6E_2 \rightarrow E_2}
& 
\begin{bmatrix}
            1  &  -3 & -3 & -2          & \bigm| & 0  \\
            0  &  1  &  2 & 5/6         & \bigm| & 0  \\
            0  &  0  &  0 & \frac{5}{3} & \bigm| & 0
\end{bmatrix}

&
\rightarrow \begin{bmatrix}
            1  & -3  & -3 & -2  & \bigm| & 0  \\
            0  &  1  & 2  & 5/6 & \bigm| & 0  \\
            0  &  0  & 0  & 1   & \bigm| & 0
\end{bmatrix}
\end{array}
\end{equation*}

\begin{equation*}
\begin{array}{ccc}
  x_4                         & = & 0 \\
  x_2 + 2x_3 + \frac{5}{6}(0) & = & 0 \\
  x_1 - 3x_2 - 3x_3 -2(0)     & = & 0
\end{array}
\end{equation*}

Entonces $x_4= 0$, $x_2 + 2x_3 = 0$ entonces $x_2 = -2x_3$, $x_1 = 3x_2 + 3x_3= 3(-2x_3) + 3 x_3 = - 3x_3$. Por tanto, si el vector 

\begin{equation*}
\begin{pmatrix}
 x_1 \\ x_2 \\ x_3\\ x_4 
\end{pmatrix}  =
\begin{pmatrix}
 -3 x_3 \\ -2 x_3 \\ x_3\\ 0 
\end{pmatrix}   = x_3 \cdot 
\begin{pmatrix}
 -3 \\ -2  \\ 1 \\ 0 
\end{pmatrix} 
\end{equation*}

Por tanto el conjunto solución 
\[
\mathcal{X}= \begin{Bmatrix} 
\begin{pmatrix}
 x_1 \\ x_2  \\ x_3 \\ x_4 
\end{pmatrix} \in \mathbb{R}^{4} \,:\,\, \begin{pmatrix}
 x_1 \\ x_2  \\ x_3 \\ x_4 
\end{pmatrix} = t \cdot \begin{pmatrix}
 -3 \\ -2  \\ 1 \\ 0 
\end{pmatrix},\,\, t\in\mathbb{R}\end{Bmatrix}
\]

## Ejemplo

Considere el  sistema de ecuaciones

\begin{equation*}
  \begin{matrix}
           -x  & +3 y  & -2z & = 4  \\
           -x  & +4 y  & -3z & = 5   \\
           -x  & +5 y  & -4z  & = 6
  \end{matrix}
\end{equation*}

La matriz extendida del sistema es

\begin{equation*}
  \begin{bmatrix}
      - 1   & 3   & -2  & \bigm| & 4   \\
      - 1   & 4   & -3  & \bigm| & 5    \\
      - 1   & 5   & -4  & \bigm| & 6  
  \end{bmatrix}
\end{equation*}

Aplicamos eliminación gaussiana a la matriz extendida

\begin{equation*}
\begin{array}{ccc}
\xrightarrow[-E_1 + E_2 \rightarrow E_1]{ -E_1 + E_3 \rightarrow E_3}
& 
\begin{bmatrix}
            -1 &  3  & -2  & \bigm| & 4  \\
            0  &  1  & -1  & \bigm| & 1  \\
            0  &  2  & -2  & \bigm| & 2
\end{bmatrix}

&
\rightarrow \begin{bmatrix}
            -1  &  3  & -2  &  \bigm| & 4  \\
            0   &  1  & -1  &   \bigm| & 1  \\
            0   &  0  & 0   &   \bigm| & 0
\end{bmatrix}
\end{array}
\end{equation*}

El último renglón se ha anulado

\begin{equation*}
\begin{array}{ccc}
 -x_1 + 3x_2 -2x_3 & = & 4 \\
      x_2 - 3x_3   & = & 1
\end{array}
\end{equation*}
 Entonces $x_2 = 1 + 3x_3$, $x_1  = -4 +3x_2-2x_3=-4+3(1+3x_3)-2x_3=-1+7x_3$


\[
\mathcal{X}= \begin{Bmatrix} 
\begin{pmatrix}
 x_1 \\ x_2  \\ x_3 \\  
\end{pmatrix} \in \mathbb{R}^{3} \,:\,\, \begin{pmatrix}
 x_1 \\ x_2  \\ x_3 
\end{pmatrix} = \begin{pmatrix}
 -1 \\ 1  \\ 0  
\end{pmatrix} + x_3 \cdot \begin{pmatrix}
 7 \\ 3  \\ 1  
\end{pmatrix},\,\, t\in\mathbb{R}\end{Bmatrix}
\]


Para el siguiente sistema

\begin{equation*}
  \begin{matrix}
     -2 x_1 & -4 x_2 & +2 x_3 &-6x_4     & = 0  \\
      3 x_1 &  6 x_2  & -2 x_3& +13 x_4  &= 6\\
     2 x_1 &  + 4x_2  &      &  +14 x_4  &=12\\
     4 x_1 & + 8x_2  &-7x_3 &            &= -10
  \end{matrix}
\end{equation*}

## Ejemplo

Obtener la matriz extendida del sistema, identificar el sistema triangular o escalonado.

\begin{equation*}
  \begin{matrix}
      3 x_1 & +2 x_2 & +3 x_3 &-2x_4     & = 1  \\
        x_1 & +  x_2 & +x_3   &          & = 3   \\
        x_1 & + 2x_2 & +x_3   & -x_4     & = 2
  \end{matrix}
\end{equation*}


La matriz extendida asociada al sistema es


\begin{equation*}
  \begin{bmatrix}
      3   & 2   & 3 & -2  & \bigm| & 1   \\
      1   & 1   & 1 &  0  & \bigm| & 3    \\
      1   & 2   & 1 & -1  & \bigm| & 2  
  \end{bmatrix}
\end{equation*}

Aplicando operaciones elementales, primero intercambiamos la ecuación 3 con la ecuación 1

\begin{equation*}
\begin{array}{ccc}
\xrightarrow[-E_1+E_2 \rightarrow E_2]{E_1  \rightarrow E_3}
& 
\begin{bmatrix}
            1  &  2  &  1  & -1  & \bigm| & 2  \\
            0  &  -1 &  0  & 1   & \bigm| & 3  \\
            3  &  2  &  3  &-2   & \bigm| & 1
\end{bmatrix}

&
\xrightarrow{-3E_1 + E_3  \rightarrow E_3} \begin{bmatrix}
            1  &  2  & 1   &  -1  &   \bigm| & 2  \\
            0  & -1  & 0   &   1  &   \bigm| & 1  \\
            0  & -4  & 0   &   1  &   \bigm| & -5
\end{bmatrix}
\end{array}
\end{equation*}

\begin{equation*}
\begin{array}{ccc}
\xrightarrow[-4E_2+E_3 \rightarrow E_3]{}
& 
\begin{bmatrix}
           1   &  2  & 1   &  -1  &   \bigm| & 2  \\
            0  & -1  & 0   &   1  &   \bigm| & 1  \\
            0  &  0  & 0   &  -3  &   \bigm| & -9
\end{bmatrix}
&
\xrightarrow{-E_2 ->E_2, -\frac{1}{3}E_3 -> E_3} \begin{bmatrix}
            1  &  2  & 1   &  -1  &    \bigm| & 2  \\
            0  &  1  & 0   &   -1  &   \bigm| & -1  \\
            0  &  0  & 0   &   1  &    \bigm| & 3
\end{bmatrix}
\end{array}
\end{equation*}

Eliminación gaussiana concluye aquí, la matriz resultante no es triangular, pero tiene una forma escalonada.


\begin{equation*}
\begin{array}{cc}
 U = &  \begin{bmatrix}
          1   & 2   & 1 & -1  & \bigm| & 2   \\
          0   & 1   & 0 & -1  & \bigm| & -1    \\
          0   & 0   & 0 &  1  & \bigm| & 3  
      \end{bmatrix}
\end{array}
\end{equation*}

Esta matriz es la matriz extendida del sistema
\begin{equation*}
\begin{array}{cc}
 &  \begin{matrix}
          1x_1& +2x_2  & +1x_3 & -1x_4  &  = & 2   \\
              & x_2    &       & -1x_4  &  = & -1    \\
              &        &       &  x_4   &  = & 3  
      \end{matrix}
\end{array}
\end{equation*}

Aplicando sustitución hacia atrás $x_4 = 3$, $x_2=2$, $x_1 + x_3 = 1$ o $x_1=1-x_3$. 
Por tanto, para que un vector $\vec{v}=\begin{pmatrix}  x_1 \\ x_2 \\ x_3\\ x_4 \end{pmatrix}$ sea solución al sistema, se tiene que cumplir

\[
\mathcal{X}= \begin{Bmatrix} 
\begin{pmatrix}
 x_1 \\ x_2  \\ x_3 \\  x_4
\end{pmatrix} \in \mathbb{R}^{4} \,:\,\, \begin{pmatrix}
 x_1 \\ x_2  \\ x_3 \\ x_4
\end{pmatrix} = \begin{pmatrix}
  1 \\ 2  \\ 0  \\ 3
\end{pmatrix} + t \cdot \begin{pmatrix}
 -1\\ 0  \\ 1 \\ 0 
\end{pmatrix},\,\, t\in\mathbb{R}\end{Bmatrix}
\]
Es decir, las soluciones están sobre una recta.

# Ejercicios
\small
Obtener la expresión matricial de los sistemas de ecuaciones lineales en los ejemplos de la clase.

Para el siguiente sistema

\begin{equation*}
  \begin{matrix}
      x_1   & -2 x_2  & +2 x_3  & = 0  \\
      2 x_1 &  - x_2  & +5 x_3  & = 0 \\
      x_1   &  + x_2  & +4 x_3  & = 0 
  \end{matrix}
\end{equation*}
La matriz del sistema y la matriz extendida asociada $[\mathbf{A}|\mathbf{b}]$ al sistema es 
$\mathbf{A}= ?$ y $[\mathbf{A}|b]$
\begin{equation*}
  \begin{matrix}
            &  2 x_2 &+ 3 x_3  & = 8  \\
     2 x_1  & + 3x_2 & +  x_3  & = 5   \\
      x_1   &  -x_2  &  -2 x_3  & =-5
  \end{matrix}
\end{equation*}
$[\mathbf{A} | \mathbf{b}] =?$


\begin{equation*}
  \begin{matrix}
        w   & - x  & - y  & +2z & = 1  \\
     2 w    & - 2x & - y  & +3z & = 3   \\
      -w    & + x  &  -y  &     &=-3
  \end{matrix}
\end{equation*}

$[\mathbf{A} | \mathbf{b}] =?$


# **Matrices** **y** **vectores**

\small
\small
Una matriz de _orden_ $m\times n$ es una función $\mathbf{A}:\{1,2,\ldots,m\}\times\{1,2,\ldots,n\} \rightarrow\mathbb{R}$. Que se escribe como un arreglo rectangular

\begin{equation}
\begin{array}{cc}
\mathbf{A}  =&  \begin{bmatrix}
        a_{11}  & a_{12}  & \cdots   & a_{1n}   \\
        a_{21}  & a_{22}  & \cdots   & a_{2n} \\
        \vdots   & \vdots  & \vdots   & \vdots \\
        a_{m1}  & a_{m2}  & \cdots   & a_{mn} 
  \end{bmatrix}
\end{array}
\end{equation}
\pause

- Decimos que la matriz $\mathbf{A}$ es de tamaño $m\times n$ si el arreglo tiene $m$ filas y $n$ columnas. Al número de la fila $i$ y columna $j$ es el número $a_{ij}$ se le llama elemento ${(i,j)}$ de la matriz.
\pause

- Una matriz es _cuadrada_ si $m=n$, es decir si el número de renglones es igual al número de columnas.
\pause

- Si una matriz no es cuadrada, entonces se dice que es _rectangular_.

# **Matrices**
\small
- Una matriz _diagonal_ es una matriz cuadrada cuyos elementos que no están en la diagonal principal son iguales a cero. Podemos referirnos a la matriz diagonal como $\mbox{diag}(d_1,d_2,\ldots, d_n)$.
\pause
- La matriz _identidad_ es la matriz _diagonal_ cuya diagonal principal es $\{1,\ldots,1\}$ y sus otras entradas son $0$.
\pause
\begin{equation}
\begin{array}{cc}
\mathbf{Id}  =&  \begin{bmatrix}
        1  & 0  & \ldots   & 0   \\
        0  & 1  & \ldots   & 0 \\
        \vdots  & \vdots   & \ddots   & \vdots \\
        0  & 0  & \ldots   & 1 
  \end{bmatrix}
\end{array}
\end{equation}


# **Matrices** II
\small
## Operaciones con matrices
- La matriz _cero_ $\big[0\big]_{mn}$ es la matriz con $a_{ij}=0$. \pause

La **suma** de matrices del mismo orden $A$ y $B$

\begin{equation*}
\begin{array}{cc}
\mathbf{A} + \mathbf{B}   = & 
      \begin{bmatrix}
              a_{11}+b_{11}  & a_{12}+b_{12}  & \cdots   & a_{1n}+b_{1n}   \\
              a_{21}+b_{21}  & a_{22}+b_{22}  & \cdots   & a_{2n}+b_{2n} \\
              \vdots   & \vdots  & \vdots   & \vdots \\
              a_{m1}+b_{m1}  & a_{m2}+b_{m2}  & \cdots   & a_{mn}+b_{mn} 
        \end{bmatrix}
\end{array}
\end{equation*}

La **multiplicación** **por** **escalar** $\lambda \in\mathbb{R}$.

\begin{equation*}
\begin{array}{cc}
\lambda \mathbf{A}    = & 
      \begin{bmatrix}
              \lambda a_{11}  & \lambda a_{12}  & \cdots   & \lambda a_{1n}   \\
              \lambda a_{21}  & \lambda a_{22}  & \cdots   & \lambda a_{2n} \\
              \vdots  & \vdots  & \vdots   & \vdots \\
              \lambda a_{m1}  & \lambda a_{m2}  & \cdots   & \lambda a_{mn} 
        \end{bmatrix}
\end{array}
\end{equation*}


# **Multiplicación** **de** **Matrices** 
\small
## Operaciones con matrices

Para que la multiplicación de dos matrices $A$ y $B$ esté definida, _el_ _número_ _de_ _columnas_ _de_ $A$ _debe_ _ser_ _igual_ _al_ _número_ _de_ _renglones_ _de_ $B$. Supongamos que  el orden de $A$ es $n\times m$ y $B$ tiene el orden $m\times p$.

\begin{equation}
\begin{array}{cc}
\mathbf{A} \cdot \mathbf{B}   = & 
      \begin{bmatrix}
              \sum_{k=1}^{m}a_{1k}b_{k1}  & \sum_{k=1}^{m}a_{1k}b_{k2}  & \cdots   & \sum_{k=1}^{m}a_{1k}b_{kn}   \\
              \sum_{k=1}^{m}a_{2k}b_{k1} & \sum_{k=1}^{m}a_{2k}b_{k2}  & \cdots   & \sum_{k=1}^{m}a_{2k}b_{kn} \\
              \vdots   & \vdots  & \vdots   & \vdots \\
              \sum_{k=1}^{m}a_{mk}b_{k1}  & \sum_{k=1}^{m}a_{mk}b_{k2}  & \cdots   & \sum_{k=1}^{m}a_{mk}b_{kn} 
        \end{bmatrix}
\end{array}
\end{equation}

Dicho de otra forma, las entradas de $\big( A\cdot B\big)_{ij} = \sum_{k=1}^{m}a_{ik}b_{kj}$ con $i=1,\,2,\,\ldots,n$ y $j=1,2,\ldots,p$. El producto de matrices tendrá $n-$ filas y $p-$ columnas.

# Matrices
## Propiedades de las operaciones on Matrices
\small
1.- Para toda matriz $A$ y $\alpha,\beta\in \mathbb{R}$.


* $1 \cdot \textbf{A} = \textbf{A} \cdot 1 =  \textbf{A}$.
 
* $0\cdot  \mathbf{A} = \mathbf{A}\cdot 0 =  \mathbf{0}$.
 
* $\alpha \cdot (\beta\cdot A) = (\alpha \beta)\mathbf{A}$.



2.- Si $A$, $B$ y $C$ tienen el mismo orden.

* $\mathbf{A} + (\mathbf{B} + \mathbf{C}) = (\mathbf{A} + \mathbf{B}) + \mathbf{C}$.

 * $\mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A}$.
 
 * $(\alpha+\beta)\cdot A = \alpha\cdot \mathbf{A} + \beta \cdot \mathbf{B}$.
 
 * $\alpha \cdot (\mathbf{A}+ \mathbf{B}) = \alpha\cdot \mathbf{A} + \alpha \cdot \mathbf{B}$.
 
 * $\mathbf{A}+\mathbf{0} = \mathbf{0}+\mathbf{A}=\mathbf{A}$
 
 * $-A + A= \mathbf{0}$



# Multiplicacion de Matrices
\small

3.- Si $A$, $B$ y $C$ tienen el orden adecuado.


  a) $(A+B)C = AC + BC$.
  b) $C(A+B) = CA + CB$.
  c) $A(BC) = (AB)C$.


Para ver la _asociatividad_, sea $M=AB$ y $N=BC$ queremos verificar que $AN = MC$.
Por definición de la multiplicación de matrices 
$$
\begin{array}{cc}
m_{ij} = & \sum_{k=1}^{n}a_{ik}b_{kj}. \\
n_{ij} = & \sum_{r=1}^{p}b_{ir}b_{rj}.
\end{array}
$$
Entonces 
$$
\big( MC \big)_{ij}=\sum_{l=1}^{p}m_{il}c_{lj} = \sum_{l=1}^{p} \sum_{k=1}^n a_{ik}b_{kl}c_{lj} =  \sum_{k=1}^n a_{ik} \sum_{l=1}^{p}  b_{kj}c_{lj}
$$
$$
=\sum_{k=1}^na_{ik}n_{kj}=(AN)_{ij}
$$

De la fórmula se deduce que el producto de varias matrices dispuestas en un orden determinado no dependen de qué producto se hace primero.

# Multiplicacion de Matrices
\small

Por ejemplo, por asociatividad
$$
((AB)C)D = (A(BC))D = A((BC)D)
$$
## Potencias de matrices
Sea $M$ una matriz **cuadrada** de orden $n$, por definición

\begin{equation*}
M^{0}=Id(n)\,\,M^{1} = M\,\,  M^{2} = MM\,\, M^{3} = MMM.
\end{equation*}

En general
$$
\begin{array}{cc}
M^{p}M^{q}= M^{p+q}\\
(M^{p})^{q} = M^{pq}
\end{array}
$$

_Observación_ : Las propiedades de productos notables no se conserven.

$$
\begin{array}{cccc}
(A+B)^{2} & = & (A+B)(A+B) &= A^2 + AB + BA + B^2 \\
(A+B)(A-B) &= &A(A-B) + B(A-B)& = A^2 -AB + BA -B^2
\end{array}
$$

# Matriz transpuesta

\small 
Sea $A=(a_{ij})$ una matriz de orden $m\times n$, la matriz transpuesta de $A$, $A^{T}$ es la matriz de orden $n\times m$ tal que $(A^T)_{ij}=A_{ji}$.
\begin{equation*}
\begin{array}{ccccc}
A = & \begin{pmatrix} 3 & -2  \\ -4 & 3 \end{pmatrix} &\,\,\,\, & A^{T} = & \begin{pmatrix} 3 & -4  \\ -2 & 3 \end{pmatrix}  \\
\end{array}
\end{equation*}

\begin{equation*}
\begin{array}{ccccc}
A = & \begin{pmatrix} 3 & -1 & 0 \\ -2 & 1 & 1 \\ 2 & -1 & 4 \end{pmatrix} &\,\,\,\, & A^{T} = & \begin{pmatrix}  3 & -2 & 2 \\ -1 & 1 & -1 \\ 0 & 1 & 4  \end{pmatrix}  \\
\end{array}
\end{equation*}


\begin{equation*}
\begin{array}{ccccc}
A = & \begin{pmatrix} -1 & 3 & 0 \\ -2 & 1 & 1 \\ 3 & 0 & -2 \\ 4 & 1 & 2 \end{pmatrix} &\,\,\,\, & A^{T} = & \begin{pmatrix}  -1 & -2 & 3 & 4 \\ 3 & 1 & 0 &  1 \\ 0 & 1 & -2 & 2  \end{pmatrix}  \\
\end{array}
\end{equation*}

# Propiedades de la matriz transpuesta

\begin{description}
  \item 1) $(\alpha A + \beta B)^{T} = \alpha A^{T} + \beta B^{T}$.
  \item 2) $(AB)^{T} = B^{T}A^{T}$.
\end{description}

La segunda propiedad 

\begin{equation*}
  [(AB)^T]_{ij} = \sum_{k=1}^m a_{jk}b_{ki} = \sum_{k=1}^m b^T_{ik} a^T_{kj} = [B^TA^T]_{ij}
\end{equation*}

Si $\mathbf{A}$ es una matriz cuadrada de orden $n$ y $\mathbf{A}=\mathbf{A}^{T}$ decimos que $\mathbf{A}$ es una matriz **simétrica**.
Si $\mathbf{A}=-\mathbf{A}^{T}$ decimos que es **antisimétrica**.

# Matriz conjugada y adjunta
\small
Sea $A=(a_{ij})$ una matriz de orden $m\times n$ de números complejos, la matriz **conjugada** de $A$, $\bar{A}$ es la matriz de orden $m\times n$ tal que $(\bar{A})_{ij}=\bar{A}_{ij}$.

Sea $A=(a_{ij})$ una matriz de orden $m\times n$ de números complejos, la matriz **adjunta** de $A$, $A^{*}$ es la matriz de orden $n\times m$ tal que $(A)_{ij}=(\bar{A})_{jj}$. Es decir, la adjunta de $A$ se obtiene al _conjugar_ y _transponer_ $A$.

\begin{equation*}
\begin{array}{ccccc}
A = & \begin{pmatrix} -i & 2 + 3i  \\ -4i & 5+2i \end{pmatrix} &\,\,\,\, & A^{*} = & \begin{pmatrix} i & 4i  \\ 2-3i & 5-2i \end{pmatrix}  \\
\end{array}
\end{equation*}

Si $\mathbf{A}$ es una matriz cuadrada y $A$ es igual a su adjunta, entonces se dice que $\mathbf{A}$ es una matriz **hermitiana** o **simétrica** **según** **Hermite**.

# Matriz Inversa
\small 
Si $\mathbf{A}$ es una matriz cuadrada de orden $n$, se dice que $X$ es la matriz **inversa** de $A$ si 
$$
A\cdot X = X\cdot A = \mathbf{Id}
$$

Si dicha matriz existe decimos que $A$ es **invertible**.

